{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d076ea64-ea9f-49d6-82d2-209da4556e78",
   "metadata": {
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "source": [
    "# EchoMimicV2 DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a1105f-87f4-4b4c-9edf-902b503cd2b3",
   "metadata": {
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 AntGroup\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e020814-a61b-4850-896b-c09bb664ddab",
   "metadata": {
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "source": [
    "## Input Refimg, Audio, and Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b509db6-0757-4b41-b648-86debe2e8993",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "refimg_path = './assets/halfbody_demo/refimag/test.png'\n",
    "audio_path ='./assets/halfbody_demo/audio/chinese/echomimicv2_woman.wav'\n",
    "using_video_driving = False\n",
    "if not using_video_driving:\n",
    "  pose_path = './assets/halfbody_demo/pose/good'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f0acb-ae0b-4d4e-8045-e18f101da5dc",
   "metadata": {
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "source": [
    "## Align reference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05581a41-300a-4afd-9ef6-f19acb29a270",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dwpose_detector init ok cuda\n",
      "get_pose_params...\n",
      "aligned img shape: (768, 768, 3)\n"
     ]
    }
   ],
   "source": [
    "# reference image aligned\n",
    "import sys\n",
    "from src.utils.img_utils import pil_to_cv2, cv2_to_pil, center_crop_cv2, pils_from_video, save_videos_from_pils, save_video_from_cv2_list\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from IPython import embed\n",
    "import numpy as np\n",
    "import copy\n",
    "from src.utils.motion_utils import motion_sync\n",
    "import pathlib\n",
    "import torch\n",
    "import pickle\n",
    "from glob import glob\n",
    "import os\n",
    "from src.models.dwpose.dwpose_detector import dwpose_detector as dwprocessor\n",
    "from src.models.dwpose.util import draw_pose\n",
    "import decord\n",
    "from tqdm import tqdm\n",
    "from moviepy.editor import AudioFileClip, VideoFileClip\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "##################################\n",
    "process_num = 100 #1266\n",
    "\n",
    "start = 0\n",
    "end = process_num + start\n",
    "#################################\n",
    "MAX_SIZE = 768\n",
    "\n",
    "def convert_fps(src_path, tgt_path, tgt_fps=24, tgt_sr=16000):\n",
    "    clip = VideoFileClip(src_path)\n",
    "    new_clip = clip.set_fps(tgt_fps)\n",
    "    if tgt_fps is not None:\n",
    "        audio = new_clip.audio\n",
    "        audio = audio.set_fps(tgt_sr)\n",
    "        new_clip = new_clip.set_audio(audio)\n",
    "    if '.mov' in tgt_path:\n",
    "        tgt_path = tgt_path.replace('.mov', '.mp4')\n",
    "    new_clip.write_videofile(tgt_path, codec='libx264', audio_codec='aac')\n",
    "\n",
    "def get_video_pose(\n",
    "        video_path: str,\n",
    "        sample_stride: int=1,\n",
    "        max_frame=None):\n",
    "\n",
    "    # read input video\n",
    "    vr = decord.VideoReader(video_path, ctx=decord.cpu(0))\n",
    "    sample_stride *= max(1, int(vr.get_avg_fps() / 24))\n",
    "\n",
    "    frames = vr.get_batch(list(range(0, len(vr), sample_stride))).asnumpy()\n",
    "    # print(frames[0])\n",
    "    if max_frame is not None:\n",
    "        frames = frames[0:max_frame,:,:]\n",
    "    height, width, _ = frames[0].shape\n",
    "    detected_poses = [dwprocessor(frm) for frm in frames]\n",
    "    dwprocessor.release_memory()\n",
    "\n",
    "    return detected_poses, height, width, frames\n",
    "\n",
    "def resize_and_pad(img, max_size):\n",
    "    img_new = np.zeros((max_size, max_size, 3)).astype('uint8')\n",
    "    imh, imw = img.shape[0], img.shape[1]\n",
    "    half = max_size // 2\n",
    "    if imh > imw:\n",
    "        imh_new = max_size\n",
    "        imw_new = int(round(imw/imh * imh_new))\n",
    "        half_w = imw_new // 2\n",
    "        rb, re = 0, max_size\n",
    "        cb = half-half_w\n",
    "        ce = cb + imw_new\n",
    "    else:\n",
    "        imw_new = max_size\n",
    "        imh_new = int(round(imh/imw * imw_new))\n",
    "        half_h = imh_new // 2\n",
    "        cb, ce = 0, max_size\n",
    "        rb = half-half_h\n",
    "        re = rb + imh_new\n",
    "\n",
    "    img_resize = cv2.resize(img, (imw_new, imh_new))\n",
    "    img_new[rb:re,cb:ce,:] = img_resize\n",
    "    return img_new\n",
    "\n",
    "def resize_and_pad_param(imh, imw, max_size):\n",
    "    half = max_size // 2\n",
    "    if imh > imw:\n",
    "        imh_new = max_size\n",
    "        imw_new = int(round(imw/imh * imh_new))\n",
    "        half_w = imw_new // 2\n",
    "        rb, re = 0, max_size\n",
    "        cb = half-half_w\n",
    "        ce = cb + imw_new\n",
    "    else:\n",
    "        imw_new = max_size\n",
    "        imh_new = int(round(imh/imw * imw_new))\n",
    "        imh_new = max_size\n",
    "\n",
    "        half_h = imh_new // 2\n",
    "        cb, ce = 0, max_size\n",
    "        rb = half-half_h\n",
    "        re = rb + imh_new\n",
    "\n",
    "    return imh_new, imw_new, rb, re, cb, ce\n",
    "\n",
    "def get_pose_params(detected_poses, max_size):\n",
    "    print('get_pose_params...')\n",
    "    # pose rescale\n",
    "    w_min_all, w_max_all, h_min_all, h_max_all = [], [], [], []\n",
    "    mid_all = []\n",
    "    for num, detected_pose in enumerate(detected_poses):\n",
    "        detected_poses[num]['num'] = num\n",
    "        candidate_body = detected_pose['bodies']['candidate']\n",
    "        score_body = detected_pose['bodies']['score']\n",
    "        candidate_face = detected_pose['faces']\n",
    "        score_face = detected_pose['faces_score']\n",
    "        candidate_hand = detected_pose['hands']\n",
    "        score_hand = detected_pose['hands_score']\n",
    "\n",
    "        # face\n",
    "        if candidate_face.shape[0] > 1:\n",
    "            index = 0\n",
    "            candidate_face = candidate_face[index]\n",
    "            score_face = score_face[index]\n",
    "            detected_poses[num]['faces'] = candidate_face.reshape(1, candidate_face.shape[0], candidate_face.shape[1])\n",
    "            detected_poses[num]['faces_score'] = score_face.reshape(1, score_face.shape[0])\n",
    "        else:\n",
    "            candidate_face = candidate_face[0]\n",
    "            score_face = score_face[0]\n",
    "\n",
    "        # body\n",
    "        if score_body.shape[0] > 1:\n",
    "            tmp_score = []\n",
    "            for k in range(0, score_body.shape[0]):\n",
    "                tmp_score.append(score_body[k].mean())\n",
    "            index = np.argmax(tmp_score)\n",
    "            candidate_body = candidate_body[index*18:(index+1)*18,:]\n",
    "            score_body = score_body[index]\n",
    "            score_hand = score_hand[(index*2):(index*2+2),:]\n",
    "            candidate_hand = candidate_hand[(index*2):(index*2+2),:,:]\n",
    "        else:\n",
    "            score_body = score_body[0]\n",
    "        all_pose = np.concatenate((candidate_body, candidate_face))\n",
    "        all_score = np.concatenate((score_body, score_face))\n",
    "        all_pose = all_pose[all_score>0.8]\n",
    "\n",
    "        body_pose = np.concatenate((candidate_body,))\n",
    "        mid_ = body_pose[1, 0]\n",
    "\n",
    "        face_pose = candidate_face\n",
    "        hand_pose = candidate_hand\n",
    "\n",
    "        h_min, h_max = np.min(face_pose[:,1]), np.max(body_pose[:7,1])\n",
    "\n",
    "        h_ = h_max - h_min\n",
    "\n",
    "        mid_w = mid_\n",
    "        w_min = mid_w - h_ // 2\n",
    "        w_max = mid_w + h_ // 2\n",
    "\n",
    "        w_min_all.append(w_min)\n",
    "        w_max_all.append(w_max)\n",
    "        h_min_all.append(h_min)\n",
    "        h_max_all.append(h_max)\n",
    "        mid_all.append(mid_w)\n",
    "\n",
    "    w_min = np.min(w_min_all)\n",
    "    w_max = np.max(w_max_all)\n",
    "    h_min = np.min(h_min_all)\n",
    "    h_max = np.max(h_max_all)\n",
    "    mid = np.mean(mid_all)\n",
    "\n",
    "    margin_ratio = 0.25\n",
    "    h_margin = (h_max-h_min)*margin_ratio\n",
    "\n",
    "    h_min = max(h_min-h_margin*0.65, 0)\n",
    "    h_max = min(h_max+h_margin*0.05, 1)\n",
    "\n",
    "    h_new = h_max - h_min\n",
    "\n",
    "    h_min_real = int(h_min*height)\n",
    "    h_max_real = int(h_max*height)\n",
    "    mid_real = int(mid*width)\n",
    "\n",
    "    height_new = h_max_real-h_min_real+1\n",
    "    width_new = height_new\n",
    "    w_min_real = mid_real - width_new // 2\n",
    "    if w_min_real < 0:\n",
    "      w_min_real = 0\n",
    "      width_new = mid_real * 2\n",
    "\n",
    "    w_max_real = w_min_real + width_new\n",
    "    w_min = w_min_real / width\n",
    "    w_max = w_max_real / width\n",
    "\n",
    "    imh_new, imw_new, rb, re, cb, ce = resize_and_pad_param(height_new, width_new, max_size)\n",
    "    res = {'draw_pose_params': [imh_new, imw_new, rb, re, cb, ce],\n",
    "           'pose_params': [w_min, w_max, h_min, h_max],\n",
    "           'video_params': [h_min_real, h_max_real, w_min_real, w_max_real],\n",
    "           }\n",
    "    return res\n",
    "\n",
    "def save_pose_params_item(input_items):\n",
    "    detected_pose, pose_params, draw_pose_params, save_dir = input_items\n",
    "    w_min, w_max, h_min, h_max = pose_params\n",
    "    num = detected_pose['num']\n",
    "    candidate_body = detected_pose['bodies']['candidate']\n",
    "    candidate_face = detected_pose['faces'][0]\n",
    "    candidate_hand = detected_pose['hands']\n",
    "    candidate_body[:,0] = (candidate_body[:,0]-w_min)/(w_max-w_min)\n",
    "    candidate_body[:,1] = (candidate_body[:,1]-h_min)/(h_max-h_min)\n",
    "    candidate_face[:,0] = (candidate_face[:,0]-w_min)/(w_max-w_min)\n",
    "    candidate_face[:,1] = (candidate_face[:,1]-h_min)/(h_max-h_min)\n",
    "    candidate_hand[:,:,0] = (candidate_hand[:,:,0]-w_min)/(w_max-w_min)\n",
    "    candidate_hand[:,:,1] = (candidate_hand[:,:,1]-h_min)/(h_max-h_min)\n",
    "    detected_pose['bodies']['candidate'] = candidate_body\n",
    "    detected_pose['faces'] = candidate_face.reshape(1, candidate_face.shape[0], candidate_face.shape[1])\n",
    "    detected_pose['hands'] = candidate_hand\n",
    "    detected_pose['draw_pose_params'] = draw_pose_params\n",
    "    np.save(save_dir+'/'+str(num)+'.npy', detected_pose)\n",
    "\n",
    "def save_pose_params(detected_poses, pose_params, draw_pose_params, ori_video_path):\n",
    "    save_dir = ori_video_path.replace('video', 'pose/')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    input_list = []\n",
    "\n",
    "    for i, detected_pose in enumerate(detected_poses):\n",
    "        input_list.append([detected_pose, pose_params, draw_pose_params, save_dir])\n",
    "\n",
    "    pool = ThreadPool(8)\n",
    "    pool.map(save_pose_params_item, input_list)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return save_dir\n",
    "from torchvision.transforms import functional as F\n",
    "def get_img_pose(\n",
    "        img_path: str,\n",
    "        sample_stride: int=1,\n",
    "        max_frame=None):\n",
    "\n",
    "  # read input img\n",
    "  frame = cv2.imread(img_path)\n",
    "  height, width, _ = frame.shape\n",
    "  short_size = min(height, width)\n",
    "  resize_ratio = max(MAX_SIZE / short_size, 1.0)\n",
    "  frame = cv2.resize(frame, (int(resize_ratio * width), int(resize_ratio * height)))\n",
    "  height, width, _ = frame.shape\n",
    "  detected_poses = [dwprocessor(frame)]\n",
    "  dwprocessor.release_memory()\n",
    "\n",
    "  return detected_poses, height, width, frame\n",
    "\n",
    "def save_aligned_img(ori_frame, video_params, max_size):\n",
    "  h_min_real, h_max_real, w_min_real, w_max_real = video_params\n",
    "  img = ori_frame[h_min_real:h_max_real,w_min_real:w_max_real,:]\n",
    "  img_aligened = resize_and_pad(img, max_size=max_size)\n",
    "  print('aligned img shape:', img_aligened.shape)\n",
    "  save_dir = './assets/refimg_aligned'\n",
    "\n",
    "  os.makedirs(save_dir, exist_ok=True)\n",
    "  save_path = os.path.join(save_dir, 'aligned.png')\n",
    "  cv2.imwrite(save_path, img_aligened)\n",
    "  return save_path\n",
    "\n",
    "detected_poses, height, width, ori_frame = get_img_pose(refimg_path, max_frame=None)\n",
    "res_params = get_pose_params(detected_poses, MAX_SIZE)\n",
    "refimg_aligned_path = save_aligned_img(ori_frame, res_params['video_params'], MAX_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3a34f9",
   "metadata": {},
   "source": [
    "## Input driving video if used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43d58644-7618-4ebb-8358-eb5afa2d5dfd",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "video_dir = './assets/halfbody_demo/'\n",
    "video_name = 'video'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13abf99-6af2-4d57-a348-99020e23b3f1",
   "metadata": {
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "source": [
    "## Extract pose from driving video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987e61f8-ddd7-48b1-8738-6b404ac7a6dc",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if using_video_driving:\n",
    "  base_dir = video_dir\n",
    "  tasks = [video_name]\n",
    "  visualization = False\n",
    "  for sub_task in tasks:\n",
    "    ori_list = os.listdir(base_dir+sub_task)\n",
    "    new_dir = base_dir + sub_task+'_24fps'\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "    index = 1\n",
    "    for i, mp4_file in enumerate(ori_list):\n",
    "      ori_video_path = base_dir + sub_task+'/'+mp4_file\n",
    "      if ori_video_path[-3:]=='mp4':\n",
    "        try:\n",
    "          # convert to 24fps\n",
    "          ori_video_path_new = ori_video_path.replace(sub_task, sub_task+'_24fps')\n",
    "          if '.MOV' in ori_video_path_new:\n",
    "              ori_video_path_new.replace('.MOV', '.mp4')\n",
    "          convert_fps(ori_video_path, ori_video_path_new)\n",
    "          # extract pose\n",
    "          detected_poses, height, width, ori_frames = get_video_pose(ori_video_path_new, max_frame=None)\n",
    "          # parameterize pose\n",
    "          res_params = get_pose_params(detected_poses, MAX_SIZE)\n",
    "          # save pose to npy\n",
    "          pose_path = save_pose_params(detected_poses, res_params['pose_params'], res_params['draw_pose_params'], ori_video_path)\n",
    "\n",
    "          index += 1\n",
    "\n",
    "        except:\n",
    "          print(\"extract crash!\")\n",
    "          continue\n",
    "\n",
    "    print([\"All Finished\", sub_task, start, end])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f8ba95-dc2f-4d85-8834-8259933ca8d5",
   "metadata": {
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "948cb9bf-ea25-459c-b7a0-096093e3f038",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Dev\\Rikkei_Avatar_Chat\\echomimic_v2\\.venv\\Lib\\site-packages\\xformers\\__init__.py\", line 57, in _is_triton_available\n",
      "    import triton  # noqa\n",
      "    ^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'triton'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add ffmpeg to path\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, DDIMScheduler\n",
    "from einops import repeat\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "from src.models.unet_2d_condition import UNet2DConditionModel\n",
    "from src.models.unet_3d_emo import EMOUNet3DConditionModel\n",
    "from src.models.whisper.audio2feature import load_audio_model\n",
    "from src.pipelines.pipeline_echomimicv2 import EchoMimicV2Pipeline\n",
    "from src.utils.util import save_videos_grid\n",
    "from src.models.pose_encoder import PoseEncoder\n",
    "from src.utils.dwpose_util import draw_pose_select_v2\n",
    "\n",
    "from decord import VideoReader\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "\n",
    "os.environ['FFMPEG_PATH'] = './ffmpeg-4.4-amd64-static'\n",
    "ffmpeg_path = os.getenv('FFMPEG_PATH')\n",
    "\n",
    "if ffmpeg_path is None:\n",
    "    print(\"please download ffmpeg-static and export to FFMPEG_PATH. \\nFor example: export FFMPEG_PATH=./ffmpeg-4.4-amd64-static\")\n",
    "elif ffmpeg_path not in os.getenv('PATH'):\n",
    "    print(\"add ffmpeg to path\")\n",
    "    os.environ[\"PATH\"] = f\"{ffmpeg_path}:{os.environ['PATH']}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8cf647",
   "metadata": {},
   "source": [
    "## Initialize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5c7173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    config = \"./configs/prompts/infer.yaml\"\n",
    "    W = 768\n",
    "    H = 768\n",
    "    # H = 512\n",
    "    # W = 512\n",
    "    L = 24\n",
    "    seed = 3407\n",
    "    context_frames = 12\n",
    "    context_overlap = 3\n",
    "    cfg = 2.5\n",
    "    steps = 30\n",
    "    sample_rate = 16000\n",
    "    fps = 12\n",
    "    # device = \"cuda\"\n",
    "    device = \"cpu\"\n",
    "    ref_images_dir = './assets/halfbody_demo/refimag'\n",
    "    pose_dir = None\n",
    "    refimg_name = 'natural_bk_openhand/0035.png'\n",
    "    pose_name = \"01\"\n",
    "    video_dir = \"./assets/halfbody_demo/video\"\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe46b83e-4e14-446e-bdde-01f663629d30",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--config\", type=str, default=\"./configs/prompts/infer.yaml\")\n",
    "# parser.add_argument(\"-W\", type=int, default=768)\n",
    "# parser.add_argument(\"-H\", type=int, default=768)\n",
    "# parser.add_argument(\"-L\", type=int, default=240)\n",
    "# parser.add_argument(\"--seed\", type=int, default=3407)\n",
    "\n",
    "# parser.add_argument(\"--context_frames\", type=int, default=12)\n",
    "# parser.add_argument(\"--context_overlap\", type=int, default=3)\n",
    "\n",
    "# parser.add_argument(\"--cfg\", type=float, default=2.5)\n",
    "# parser.add_argument(\"--steps\", type=int, default=30)\n",
    "# parser.add_argument(\"--sample_rate\", type=int, default=16000)\n",
    "# parser.add_argument(\"--fps\", type=int, default=24)\n",
    "# parser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
    "# parser.add_argument(\"--ref_images_dir\", type=str, default=f'./assets/halfbody_demo/refimag')\n",
    "# parser.add_argument(\"--pose_dir\", type=str, default=None)\n",
    "# parser.add_argument(\"--refimg_name\", type=str, default='natural_bk_openhand/0035.png')\n",
    "# parser.add_argument(\"--pose_name\", type=str, default=\"01\")\n",
    "# parser.add_argument(\"--video_dir\", type=str, default=\"./assets/halfbody_demo/video\")\n",
    "\n",
    "# args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcfc9840-8390-4b78-8365-a53db438bcc9",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs\\pretrained_weights-itermotion_module-seed3407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ./pretrained_weights/sd-image-variations-diffusers/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ./pretrained_weights/sd-image-variations-diffusers/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Some weights of the model checkpoint were not used when initializing UNet2DConditionModel: \n",
      " ['down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight, down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias, down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight, down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias, down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight, down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias, down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight, down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias, down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight, down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias, down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight, down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias, up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight, up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias, up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight, up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias, up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight, up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias, up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight, up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias, up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight, up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias, up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight, up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias, up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight, up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias, up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight, up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias, up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight, up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias, mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight, mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight, mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight, mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, mid_block.attentions.0.transformer_blocks.0.norm2.weight, mid_block.attentions.0.transformer_blocks.0.norm2.bias, conv_norm_out.weight, conv_norm_out.bias, conv_out.weight, conv_out.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using motion module\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n"
     ]
    }
   ],
   "source": [
    "config = OmegaConf.load(args.config)\n",
    "if config.weight_dtype == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "else:\n",
    "    weight_dtype = torch.float32\n",
    "\n",
    "device = args.device\n",
    "if device.__contains__(\"cuda\") and not torch.cuda.is_available():\n",
    "    device = \"cpu\"\n",
    "\n",
    "inference_config_path = config.inference_config\n",
    "infer_config = OmegaConf.load(inference_config_path)\n",
    "\n",
    "model_flag = '{}-iter{}'.format(config.motion_module_path.split('/')[-2], config.motion_module_path.split('/')[-1].split('-')[-1][:-4])\n",
    "save_dir = Path(f\"outputs/{model_flag}-seed{args.seed}/\")\n",
    "save_dir.mkdir(exist_ok=True, parents=True)\n",
    "print(save_dir)\n",
    "\n",
    "############# model_init started #############\n",
    "## vae init\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    config.pretrained_vae_path,).to(device, dtype=weight_dtype)\n",
    "\n",
    "## reference net init\n",
    "reference_unet = UNet2DConditionModel.from_pretrained(\n",
    "    config.pretrained_unet_path,\n",
    "    subfolder=\"unet\",).to(dtype=weight_dtype, device=device)\n",
    "reference_unet.load_state_dict(\n",
    "    torch.load(config.reference_unet_path, map_location=\"cpu\"),)\n",
    "\n",
    "## denoising net init\n",
    "if os.path.exists(config.motion_module_path):\n",
    "    print('using motion module')\n",
    "else:\n",
    "    exit(\"motion module not found\")\n",
    "    ### stage1 + stage2\n",
    "denoising_unet = EMOUNet3DConditionModel.from_pretrained_2d(\n",
    "    config.pretrained_base_model_path,\n",
    "    config.motion_module_path,\n",
    "    subfolder=\"unet\",\n",
    "    unet_additional_kwargs=infer_config.unet_additional_kwargs,).to(dtype=weight_dtype, device=device)\n",
    "\n",
    "denoising_unet.load_state_dict(\n",
    "    torch.load(config.denoising_unet_path, map_location=\"cpu\"),\n",
    "    strict=False)\n",
    "\n",
    "# pose net init\n",
    "pose_net = PoseEncoder(320, conditioning_channels=3, block_out_channels=(16, 32, 96, 256)).to(\n",
    "    dtype=weight_dtype, device=device)\n",
    "pose_net.load_state_dict(torch.load(config.pose_encoder_path))\n",
    "\n",
    "### load audio processor params\n",
    "audio_processor = load_audio_model(model_path=config.audio_model_path, device=device)\n",
    "\n",
    "############# model_init finished #############\n",
    "width, height = 768, 768 # fixed size\n",
    "sched_kwargs = OmegaConf.to_container(infer_config.noise_scheduler_kwargs)\n",
    "scheduler = DDIMScheduler(**sched_kwargs)\n",
    "\n",
    "pipe = EchoMimicV2Pipeline(\n",
    "    vae=vae,\n",
    "    reference_unet=reference_unet,\n",
    "    denoising_unet=denoising_unet,\n",
    "    audio_guider=audio_processor,\n",
    "    pose_encoder=pose_net,\n",
    "    scheduler=scheduler,)\n",
    "\n",
    "pipe = pipe.to(device, dtype=weight_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb3af19-2df5-4923-b4bf-4402773d864d",
   "metadata": {
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "source": [
    "## Animating half-body human video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13234468-fac0-49d0-b4cf-8fe9666e3a68",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose: ./assets/halfbody_demo/pose/good\n",
      "Reference: ./assets/refimg_aligned\\aligned.png\n",
      "Audio: ./assets/halfbody_demo/audio/chinese/echomimicv2_woman.wav\n",
      "video in 12 FPS, audio idx in 50FPS\n",
      "latents shape:torch.Size([1, 4, 74, 96, 96]), video_length:74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [58:59<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     48\u001b[39m width, height = \u001b[32m768\u001b[39m, \u001b[32m768\u001b[39m\n\u001b[32m     49\u001b[39m audio_clip = audio_clip.set_duration(args.L / final_fps)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m video = \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref_image_pil\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposes_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43maudio_sample_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontext_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfinal_fps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontext_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m)\u001b[49m.videos\n\u001b[32m     67\u001b[39m final_length = \u001b[38;5;28mmin\u001b[39m(video.shape[\u001b[32m2\u001b[39m], poses_tensor.shape[\u001b[32m2\u001b[39m], args.L)\n\u001b[32m     68\u001b[39m video_sig = video[:, :, :final_length, :, :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev\\Rikkei_Avatar_Chat\\echomimic_v2\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev\\Rikkei_Avatar_Chat\\echomimic_v2\\src\\pipelines\\pipeline_echomimicv2.py:581\u001b[39m, in \u001b[36mEchoMimicV2Pipeline.__call__\u001b[39m\u001b[34m(self, ref_image, audio_path, poses_tensor, width, height, video_length, num_inference_steps, guidance_scale, num_images_per_prompt, eta, generator, output_type, return_dict, callback, callback_steps, context_schedule, context_frames, context_stride, context_overlap, context_batch_size, interpolation_factor, audio_sample_rate, fps, audio_margin, start_idx, **kwargs)\u001b[39m\n\u001b[32m    576\u001b[39m latent_model_input = \u001b[38;5;28mself\u001b[39m.scheduler.scale_model_input(\n\u001b[32m    577\u001b[39m     latent_model_input, t\n\u001b[32m    578\u001b[39m )\n\u001b[32m    579\u001b[39m b, c, f, h, w = latent_model_input.shape\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdenoising_unet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m    \u001b[49m\u001b[43maudio_cond_fea\u001b[49m\u001b[43m=\u001b[49m\u001b[43maudio_latents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43maudio_latents_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m    \u001b[49m\u001b[43mface_musk_fea\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpose_latents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpose_latents_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(new_context):\n\u001b[32m    591\u001b[39m     noise_pred[:, :, c] = noise_pred[:, :, c] + pred\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev\\Rikkei_Avatar_Chat\\echomimic_v2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev\\Rikkei_Avatar_Chat\\echomimic_v2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev\\Rikkei_Avatar_Chat\\echomimic_v2\\src\\models\\unet_3d_emo.py:495\u001b[39m, in \u001b[36mEMOUNet3DConditionModel.forward\u001b[39m\u001b[34m(self, sample, timestep, encoder_hidden_states, class_labels, audio_cond_fea, attention_mask, face_musk_fea, down_block_additional_residuals, mid_block_additional_residual, return_dict)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m downsample_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.down_blocks:\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    492\u001b[39m         \u001b[38;5;28mhasattr\u001b[39m(downsample_block, \u001b[33m\"\u001b[39m\u001b[33mhas_cross_attention\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    493\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m downsample_block.has_cross_attention\n\u001b[32m    494\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m         sample, res_samples = \u001b[43mdownsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m=\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m            \u001b[49m\u001b[43maudio_cond_fea\u001b[49m\u001b[43m=\u001b[49m\u001b[43maudio_cond_fea\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    503\u001b[39m         sample, res_samples = downsample_block(\n\u001b[32m    504\u001b[39m             hidden_states=sample,\n\u001b[32m    505\u001b[39m             temb=emb,\n\u001b[32m    506\u001b[39m             encoder_hidden_states=encoder_hidden_states,\n\u001b[32m    507\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev\\Rikkei_Avatar_Chat\\echomimic_v2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev\\Rikkei_Avatar_Chat\\echomimic_v2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev\\Rikkei_Avatar_Chat\\echomimic_v2\\src\\models\\unet_3d_blocks.py:446\u001b[39m, in \u001b[36mCrossAttnDownBlock3D.forward\u001b[39m\u001b[34m(self, hidden_states, temb, encoder_hidden_states, audio_cond_fea, attention_mask)\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    445\u001b[39m     hidden_states = resnet(hidden_states, temb)\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m     hidden_states = \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43maudio_cond_fea\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_cond_fea\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.sample\n\u001b[32m    452\u001b[39m     \u001b[38;5;66;03m# add motion module\u001b[39;00m\n\u001b[32m    453\u001b[39m     hidden_states = (\n\u001b[32m    454\u001b[39m         motion_module(\n\u001b[32m    455\u001b[39m             hidden_states, temb, encoder_hidden_states=encoder_hidden_states\n\u001b[32m   (...)\u001b[39m\u001b[32m    458\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m hidden_states\n\u001b[32m    459\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev\\Rikkei_Avatar_Chat\\echomimic_v2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev\\Rikkei_Avatar_Chat\\echomimic_v2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev\\Rikkei_Avatar_Chat\\echomimic_v2\\src\\models\\transformer_3d.py:153\u001b[39m, in \u001b[36mTransformer3DModel.forward\u001b[39m\u001b[34m(self, hidden_states, encoder_hidden_states, audio_cond_fea, timestep, return_dict)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# Blocks\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.transformer_blocks):\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     hidden_states = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43maudio_cond_fea\u001b[49m\u001b[43m=\u001b[49m\u001b[43maudio_cond_fea\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvideo_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# Output\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_linear_projection:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev\\Rikkei_Avatar_Chat\\echomimic_v2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev\\Rikkei_Avatar_Chat\\echomimic_v2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev\\Rikkei_Avatar_Chat\\echomimic_v2\\src\\models\\mutual_self_attention.py:180\u001b[39m, in \u001b[36mReferenceAttentionControl.register_reference_hooks.<locals>.hacked_basic_transformer_inner_forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, audio_cond_fea, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, video_length, audio_feature_ratio)\u001b[39m\n\u001b[32m    169\u001b[39m         _uc_mask = (\n\u001b[32m    170\u001b[39m             torch.Tensor(\n\u001b[32m    171\u001b[39m                 [\u001b[32m1\u001b[39m] * (hidden_states.shape[\u001b[32m0\u001b[39m] // \u001b[32m2\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m    175\u001b[39m             .bool()\n\u001b[32m    176\u001b[39m         )\n\u001b[32m    177\u001b[39m     \u001b[38;5;66;03m# print(hidden_states_c.shape, norm_hidden_states.shape, hidden_states.shape, _uc_mask.shape)\u001b[39;00m\n\u001b[32m    178\u001b[39m     hidden_states_c[_uc_mask] = (\n\u001b[32m    179\u001b[39m         \u001b[38;5;28mself\u001b[39m.attn1(\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m             \u001b[43mnorm_hidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_uc_mask\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[32m    181\u001b[39m             encoder_hidden_states=norm_hidden_states[_uc_mask], \u001b[38;5;66;03m# B * 4096 * 768\u001b[39;00m\n\u001b[32m    182\u001b[39m             attention_mask=attention_mask,\n\u001b[32m    183\u001b[39m         )\n\u001b[32m    184\u001b[39m         + hidden_states[_uc_mask]\n\u001b[32m    185\u001b[39m     )\n\u001b[32m    186\u001b[39m     hidden_states = hidden_states_c.clone()\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: indices should be either on cpu or on the same device as the indexed tensor (cpu)"
     ]
    }
   ],
   "source": [
    "if args.seed is not None and args.seed > -1:\n",
    "  generator = torch.manual_seed(args.seed)\n",
    "else:\n",
    "  generator = torch.manual_seed(random.randint(100, 1000000))\n",
    "\n",
    "final_fps = args.fps\n",
    "\n",
    "inputs_dict = {\n",
    "    \"refimg\": refimg_aligned_path,\n",
    "    \"audio\": audio_path,\n",
    "    \"pose\": pose_path,\n",
    "}\n",
    "\n",
    "start_idx = 0\n",
    "\n",
    "print('Pose:', inputs_dict['pose'])\n",
    "print('Reference:', inputs_dict['refimg'])\n",
    "print('Audio:', inputs_dict['audio'])\n",
    "audio_name = inputs_dict['audio'].split('/')[-1].split('.')[0]\n",
    "\n",
    "ref_flag = '.'.join([inputs_dict['refimg'].split('/')[-2], inputs_dict['refimg'].split('/')[-1]])\n",
    "save_path = Path(f\"outputs\")\n",
    "\n",
    "save_path.mkdir(exist_ok=True, parents=True)\n",
    "ref_s = inputs_dict['refimg'].split('/')[-1].split('.')[0]\n",
    "save_name = f\"{save_path}/{ref_s}-a-{audio_name}-i{start_idx}\"\n",
    "\n",
    "ref_image_pil = Image.open(inputs_dict['refimg']).resize((args.W, args.H))\n",
    "audio_clip = AudioFileClip(inputs_dict['audio'])\n",
    "\n",
    "args.L = min(int(audio_clip.duration * final_fps), len(os.listdir(inputs_dict['pose'])))\n",
    "\n",
    "pose_list = []\n",
    "for index in range(start_idx, start_idx + args.L):\n",
    "    tgt_musk = np.zeros((args.W, args.H, 3)).astype('uint8')\n",
    "    tgt_musk_path = os.path.join(inputs_dict['pose'], \"{}.npy\".format(index))\n",
    "    detected_pose = np.load(tgt_musk_path, allow_pickle=True).tolist()\n",
    "    imh_new, imw_new, rb, re, cb, ce = detected_pose['draw_pose_params']\n",
    "    im = draw_pose_select_v2(detected_pose, imh_new, imw_new, ref_w=800)\n",
    "    im = np.transpose(np.array(im),(1, 2, 0))\n",
    "    tgt_musk[rb:re,cb:ce,:] = im\n",
    "\n",
    "    tgt_musk_pil = Image.fromarray(np.array(tgt_musk)).convert('RGB')\n",
    "    pose_list.append(torch.Tensor(np.array(tgt_musk_pil)).to(dtype=weight_dtype, device=device).permute(2,0,1) / 255.0)\n",
    "\n",
    "poses_tensor = torch.stack(pose_list, dim=1).unsqueeze(0)\n",
    "audio_clip = AudioFileClip(inputs_dict['audio'])\n",
    "width, height = 768, 768\n",
    "audio_clip = audio_clip.set_duration(args.L / final_fps)\n",
    "video = pipe(\n",
    "    ref_image_pil,\n",
    "    inputs_dict['audio'],\n",
    "    poses_tensor[:,:,:args.L,...],\n",
    "    width,\n",
    "    height,\n",
    "    args.L,\n",
    "    args.steps,\n",
    "    args.cfg,\n",
    "    generator=generator,\n",
    "    audio_sample_rate=args.sample_rate,\n",
    "    context_frames=args.context_frames,\n",
    "    fps=final_fps,\n",
    "    context_overlap=args.context_overlap,\n",
    "    start_idx=start_idx,\n",
    ").videos\n",
    "\n",
    "final_length = min(video.shape[2], poses_tensor.shape[2], args.L)\n",
    "video_sig = video[:, :, :final_length, :, :]\n",
    "\n",
    "save_videos_grid(\n",
    "    video_sig,\n",
    "    save_name + \"_woa_sig.mp4\",\n",
    "    n_rows=1,\n",
    "    fps=final_fps,\n",
    ")\n",
    "\n",
    "video_clip_sig = VideoFileClip(save_name + \"_woa_sig.mp4\",)\n",
    "video_clip_sig = video_clip_sig.set_audio(audio_clip)\n",
    "video_clip_sig.write_videofile(save_name + \"_sig.mp4\", codec=\"libx264\", audio_codec=\"aac\", threads=2)\n",
    "os.system(\"rm {}\".format(save_name + \"_woa_sig.mp4\"))\n",
    "print(save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7dbb28",
   "metadata": {},
   "source": [
    "## Display video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa341db6-81ff-4cf0-8e89-41e92170a0d9",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"outputs/refimg_aligned\\aligned-a-echomimicv2_woman-i0_sig.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Video\n",
    "display(Video(filename=save_name + \"_sig.mp4\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
